{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of sung_lec01_regression(1) 2020.06.11오후수업.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sturu1/git-first/blob/master/Copy_of_sung_lec01_regression(1)_2020_06_11%EC%98%A4%ED%9B%84%EC%88%98%EC%97%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y8UEW4k5S8h",
        "colab_type": "text"
      },
      "source": [
        "## Simple Linear Regression 구현 실습(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xgpg79p5Y-e",
        "colab_type": "text"
      },
      "source": [
        "> 먼저 필요한 Keras 및 numpy 라이브러리를 불러옵니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KJ5XU2sCQUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjPo3fOE5box",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVpIAkvt5rNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxFoF8gF50ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0GjCd6E5g1n",
        "colab_type": "text"
      },
      "source": [
        "> 실습에 사용할 데이터를 numpy 배열로 생성합니다. \n",
        "총 4개의 입력 및 출력 샘플을 정의합니다. \n",
        "\n",
        "> 입력데이터는 4개의 샘플(개체)로 이루어져 있으며, 각 개체는 한개의 속성을 가지고 있습니다. 출력데이터도 4개의 샘플, 1개의 속성으로 이루어져 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8wzURqERMCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.array([[1],[2],[3],[4]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6xU4bmq6Xaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = np.array([[0],[-1],[-2],[-3]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPhUUcsj6ZE6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab573721-ee94-424e-8912-3ed347c2a809"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOlL0J07yzHL",
        "colab_type": "text"
      },
      "source": [
        "> 이러한 입력, 출력데이터간의 관계를 가장 잘 설명하는 규칙을 찾는 것이 문제입니다. simple linear regression에서는 입력과 출력 데이터 간의 관계를 선형으로 가정합니다. 즉, 다음의 방정식을 만족시키는 w1과 b 값을 찾는 것이 학습이라고 할 수 있습니다. \n",
        "``` \n",
        "y = w1*x1 + b\n",
        "```\n",
        "프로그래밍을 통해 학습하기 전에, 먼저 여러분들이 예측해 보세요. w1과 b 값은 얼마이겠습니까? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul3HfVkr5rhA",
        "colab_type": "text"
      },
      "source": [
        "각 입력 및 출력 데이터의 차원과 shape를 확인합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYdFmYKXSZAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "89e0ac29-a88c-43bb-95de-3766ba0f5208"
      },
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4, 1), (4, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt8VQOes5zv_",
        "colab_type": "text"
      },
      "source": [
        "> 이제 Keras를 이용하여 **w1**과**b** 값을 찾는 학습을 진행해 보겠습니다. 먼저 여러분은 **모델을 생성**해야 합니다. 모델을 생성한다는 것은 마치 학습을 위한 뇌의 구조를 만드는 작업에 비유할 수 있습니다. 즉, 네트워크의 구조(뇌의 뉴런의 구조), 학습을 위해 필요한 손실함수 및 optimzer 설정(Gradient Descent Algorithm)등을 정의하는 것입니다. \n",
        "\n",
        "> 복잡한 모델을 만들면 보다 복잡한 문제를 학습할 수 있지만 학습에 오랜 시간이 걸리고, 단순한 모델을 만들면 단순한 문제만 학습할 수 있지만 학습에 걸리는 시간이 짧아집니다. 마치, 특정 문제를 해결하는데 인간의 뇌를 사용할지, 강아지의 뇌를 사용할지, 곤충의 뇌를 사용할지를 정하는 것에 비유할 수 있습니다. \n",
        "\n",
        "> 이번 예제에서는 **Keras의 Sequential() 클래스**를 이용하여 네트워크의 구조를 정의합니다. 이 예제에서는 1개의 layer(층)과 하나의 셀(unit)만을 가진 매우 단순한 구조로 network를 정의하였습니다. 사실 신경망이라고 볼수도 없습니다. \n",
        "\n",
        "> 본 예제는 regression 기법을 구현하는 것으로, activation 함수를 선택하지 않았습니다. 이는 활성함수 입력 전 후로 아무런 변화가 없음을 의미합니다. 결론적으로 Dense 층이 의미하는 것은 다음과 같은 수식입니다. \n",
        "```\n",
        "y=w1 * x1 + b\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esWivA_GRN8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential() #뇌의 공간을 만들었다고생각하면됨"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoY7yXfq2COr",
        "colab_type": "text"
      },
      "source": [
        "> 먼저 keras의 **sequentiao() 클래스**를 이용하여** model 이라는 instance를 생성**하고 여기에 **layer(층)와 unit(셀, 뉴런)을 순차적으로 추가**합니다. 이때, 입력데이터의 각 개체의 모양을 input_shape로 정의하는 것이 필요합니다. input 데이터는 4개의 개체(4명의 학생)로 이루어져 있지만, 한 개체는 1개의 속성을 가지고 있으므로 **inpu_shape는 1D 벡터인 (1,)**로 지정합니다. 또는 input_dim=1 로 표현하기도 합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0925d-tV5QdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Dense(units=1, input_shape=(1,)))  #한 개체에 몇개의 속성이 잇냐 그게 중요하다"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICjqYsjFTPxA",
        "colab_type": "text"
      },
      "source": [
        "> 윗 식은 다음과 같이 표현 가능합니다. `model.add(Dense(1, input_dim=1))`\n",
        "\n",
        "> Dense 함수에서 activation 함수를 별도로 정의하지 않을 경우, 각 unit의 역할은 다음처럼 단순 선형 변환입니다. `outputput = dot(W,input)+b`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DKMSUslVtKN",
        "colab_type": "text"
      },
      "source": [
        "> 다음으로 **모델을 compile** 합니다. 이때 활용할 **손실함수**(loss function or cost function) 및 Gradient Descent 알고리즘(**optimizer**)을 정의합니다. \n",
        "본 예제는 **숫자를 예측하는 회귀분석 문제**이므로, **손실함수로 mse**(평균제곱오차)를 사용하고,**optimizer로 SGD**를 사용합니다. lr=0.1은 학습률을 정의한 것으로, 모델에 포함된 가중치가 변화하는 정도를 의미합니다. 참고로 SGD는 Gradient Descent Algorithm 중 하나로 확률적경사하강법을 의미합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liQW9s2MRRj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='mse', optimizer=SGD(lr=0.1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOQu510eVSud",
        "colab_type": "text"
      },
      "source": [
        "> 자 이제 모델의 생성이 끝났습니다. \n",
        "최종 학습 이전에 모델의 구조를 확인해 볼까요? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tDAAaAeVSLm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "fccb3233-f5c9-446b-faa3-2aafd1a0eaa4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 1)                 2         \n",
            "=================================================================\n",
            "Total params: 2\n",
            "Trainable params: 2\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xNMeDr2VnV9",
        "colab_type": "text"
      },
      "source": [
        "> 이제 입력 및 출력 샘플을 가지고 모델을 학습 시킵니다. \n",
        "> 한번에 학습되지 않겠지요? 총 200번 반복하면서 점점 학습의 정확도를 높입니다. 여기서 모든 입력데이터를 전부 사용하여 학습하는 것을 **1 epoch** 학습하였다고 표현합니다. 우리는 200번을 학습시킬 계획이므로 **epochs=200**으로 설정합니다. 학습이 진행되는 동안 model instance의 가중치는 계속 변화면서 타겟값과 예측값의 차이를 줄여가게 됩니다. 그리고 학습과정동안의 각종 지표들은 history라는 변수에 저장되게 됩니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO55DiksVlWd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c1a55b93-4a0d-4c08-fb51-c7e6a4daff08"
      },
      "source": [
        "history = model.fit(x_train, y_train, epochs=200)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 22.1725\n",
            "Epoch 2/200\n",
            "4/4 [==============================] - 0s 883us/step - loss: 10.1449\n",
            "Epoch 3/200\n",
            "4/4 [==============================] - 0s 703us/step - loss: 4.7336\n",
            "Epoch 4/200\n",
            "4/4 [==============================] - 0s 597us/step - loss: 2.2934\n",
            "Epoch 5/200\n",
            "4/4 [==============================] - 0s 241us/step - loss: 1.1879\n",
            "Epoch 6/200\n",
            "4/4 [==============================] - 0s 462us/step - loss: 0.6823\n",
            "Epoch 7/200\n",
            "4/4 [==============================] - 0s 267us/step - loss: 0.4464\n",
            "Epoch 8/200\n",
            "4/4 [==============================] - 0s 258us/step - loss: 0.3323\n",
            "Epoch 9/200\n",
            "4/4 [==============================] - 0s 890us/step - loss: 0.2733\n",
            "Epoch 10/200\n",
            "4/4 [==============================] - 0s 237us/step - loss: 0.2395\n",
            "Epoch 11/200\n",
            "4/4 [==============================] - 0s 567us/step - loss: 0.2174\n",
            "Epoch 12/200\n",
            "4/4 [==============================] - 0s 442us/step - loss: 0.2010\n",
            "Epoch 13/200\n",
            "4/4 [==============================] - 0s 265us/step - loss: 0.1875\n",
            "Epoch 14/200\n",
            "4/4 [==============================] - 0s 397us/step - loss: 0.1758\n",
            "Epoch 15/200\n",
            "4/4 [==============================] - 0s 658us/step - loss: 0.1651\n",
            "Epoch 16/200\n",
            "4/4 [==============================] - 0s 257us/step - loss: 0.1552\n",
            "Epoch 17/200\n",
            "4/4 [==============================] - 0s 251us/step - loss: 0.1460\n",
            "Epoch 18/200\n",
            "4/4 [==============================] - 0s 255us/step - loss: 0.1373\n",
            "Epoch 19/200\n",
            "4/4 [==============================] - 0s 258us/step - loss: 0.1292\n",
            "Epoch 20/200\n",
            "4/4 [==============================] - 0s 251us/step - loss: 0.1216\n",
            "Epoch 21/200\n",
            "4/4 [==============================] - 0s 372us/step - loss: 0.1144\n",
            "Epoch 22/200\n",
            "4/4 [==============================] - 0s 338us/step - loss: 0.1077\n",
            "Epoch 23/200\n",
            "4/4 [==============================] - 0s 299us/step - loss: 0.1013\n",
            "Epoch 24/200\n",
            "4/4 [==============================] - 0s 330us/step - loss: 0.0953\n",
            "Epoch 25/200\n",
            "4/4 [==============================] - 0s 329us/step - loss: 0.0897\n",
            "Epoch 26/200\n",
            "4/4 [==============================] - 0s 332us/step - loss: 0.0844\n",
            "Epoch 27/200\n",
            "4/4 [==============================] - 0s 312us/step - loss: 0.0794\n",
            "Epoch 28/200\n",
            "4/4 [==============================] - 0s 322us/step - loss: 0.0748\n",
            "Epoch 29/200\n",
            "4/4 [==============================] - 0s 348us/step - loss: 0.0703\n",
            "Epoch 30/200\n",
            "4/4 [==============================] - 0s 310us/step - loss: 0.0662\n",
            "Epoch 31/200\n",
            "4/4 [==============================] - 0s 319us/step - loss: 0.0623\n",
            "Epoch 32/200\n",
            "4/4 [==============================] - 0s 287us/step - loss: 0.0586\n",
            "Epoch 33/200\n",
            "4/4 [==============================] - 0s 293us/step - loss: 0.0552\n",
            "Epoch 34/200\n",
            "4/4 [==============================] - 0s 281us/step - loss: 0.0519\n",
            "Epoch 35/200\n",
            "4/4 [==============================] - 0s 273us/step - loss: 0.0488\n",
            "Epoch 36/200\n",
            "4/4 [==============================] - 0s 272us/step - loss: 0.0460\n",
            "Epoch 37/200\n",
            "4/4 [==============================] - 0s 338us/step - loss: 0.0433\n",
            "Epoch 38/200\n",
            "4/4 [==============================] - 0s 339us/step - loss: 0.0407\n",
            "Epoch 39/200\n",
            "4/4 [==============================] - 0s 236us/step - loss: 0.0383\n",
            "Epoch 40/200\n",
            "4/4 [==============================] - 0s 241us/step - loss: 0.0360\n",
            "Epoch 41/200\n",
            "4/4 [==============================] - 0s 229us/step - loss: 0.0339\n",
            "Epoch 42/200\n",
            "4/4 [==============================] - 0s 229us/step - loss: 0.0319\n",
            "Epoch 43/200\n",
            "4/4 [==============================] - 0s 327us/step - loss: 0.0300\n",
            "Epoch 44/200\n",
            "4/4 [==============================] - 0s 325us/step - loss: 0.0283\n",
            "Epoch 45/200\n",
            "4/4 [==============================] - 0s 279us/step - loss: 0.0266\n",
            "Epoch 46/200\n",
            "4/4 [==============================] - 0s 261us/step - loss: 0.0250\n",
            "Epoch 47/200\n",
            "4/4 [==============================] - 0s 461us/step - loss: 0.0236\n",
            "Epoch 48/200\n",
            "4/4 [==============================] - 0s 337us/step - loss: 0.0222\n",
            "Epoch 49/200\n",
            "4/4 [==============================] - 0s 365us/step - loss: 0.0209\n",
            "Epoch 50/200\n",
            "4/4 [==============================] - 0s 319us/step - loss: 0.0196\n",
            "Epoch 51/200\n",
            "4/4 [==============================] - 0s 298us/step - loss: 0.0185\n",
            "Epoch 52/200\n",
            "4/4 [==============================] - 0s 410us/step - loss: 0.0174\n",
            "Epoch 53/200\n",
            "4/4 [==============================] - 0s 276us/step - loss: 0.0164\n",
            "Epoch 54/200\n",
            "4/4 [==============================] - 0s 270us/step - loss: 0.0154\n",
            "Epoch 55/200\n",
            "4/4 [==============================] - 0s 285us/step - loss: 0.0145\n",
            "Epoch 56/200\n",
            "4/4 [==============================] - 0s 276us/step - loss: 0.0136\n",
            "Epoch 57/200\n",
            "4/4 [==============================] - 0s 276us/step - loss: 0.0128\n",
            "Epoch 58/200\n",
            "4/4 [==============================] - 0s 274us/step - loss: 0.0121\n",
            "Epoch 59/200\n",
            "4/4 [==============================] - 0s 297us/step - loss: 0.0114\n",
            "Epoch 60/200\n",
            "4/4 [==============================] - 0s 310us/step - loss: 0.0107\n",
            "Epoch 61/200\n",
            "4/4 [==============================] - 0s 282us/step - loss: 0.0101\n",
            "Epoch 62/200\n",
            "4/4 [==============================] - 0s 277us/step - loss: 0.0095\n",
            "Epoch 63/200\n",
            "4/4 [==============================] - 0s 297us/step - loss: 0.0089\n",
            "Epoch 64/200\n",
            "4/4 [==============================] - 0s 307us/step - loss: 0.0084\n",
            "Epoch 65/200\n",
            "4/4 [==============================] - 0s 352us/step - loss: 0.0079\n",
            "Epoch 66/200\n",
            "4/4 [==============================] - 0s 350us/step - loss: 0.0074\n",
            "Epoch 67/200\n",
            "4/4 [==============================] - 0s 285us/step - loss: 0.0070\n",
            "Epoch 68/200\n",
            "4/4 [==============================] - 0s 261us/step - loss: 0.0066\n",
            "Epoch 69/200\n",
            "4/4 [==============================] - 0s 288us/step - loss: 0.0062\n",
            "Epoch 70/200\n",
            "4/4 [==============================] - 0s 389us/step - loss: 0.0058\n",
            "Epoch 71/200\n",
            "4/4 [==============================] - 0s 293us/step - loss: 0.0055\n",
            "Epoch 72/200\n",
            "4/4 [==============================] - 0s 274us/step - loss: 0.0052\n",
            "Epoch 73/200\n",
            "4/4 [==============================] - 0s 305us/step - loss: 0.0048\n",
            "Epoch 74/200\n",
            "4/4 [==============================] - 0s 268us/step - loss: 0.0046\n",
            "Epoch 75/200\n",
            "4/4 [==============================] - 0s 281us/step - loss: 0.0043\n",
            "Epoch 76/200\n",
            "4/4 [==============================] - 0s 313us/step - loss: 0.0040\n",
            "Epoch 77/200\n",
            "4/4 [==============================] - 0s 357us/step - loss: 0.0038\n",
            "Epoch 78/200\n",
            "4/4 [==============================] - 0s 337us/step - loss: 0.0036\n",
            "Epoch 79/200\n",
            "4/4 [==============================] - 0s 274us/step - loss: 0.0034\n",
            "Epoch 80/200\n",
            "4/4 [==============================] - 0s 290us/step - loss: 0.0032\n",
            "Epoch 81/200\n",
            "4/4 [==============================] - 0s 291us/step - loss: 0.0030\n",
            "Epoch 82/200\n",
            "4/4 [==============================] - 0s 337us/step - loss: 0.0028\n",
            "Epoch 83/200\n",
            "4/4 [==============================] - 0s 285us/step - loss: 0.0026\n",
            "Epoch 84/200\n",
            "4/4 [==============================] - 0s 283us/step - loss: 0.0025\n",
            "Epoch 85/200\n",
            "4/4 [==============================] - 0s 287us/step - loss: 0.0023\n",
            "Epoch 86/200\n",
            "4/4 [==============================] - 0s 274us/step - loss: 0.0022\n",
            "Epoch 87/200\n",
            "4/4 [==============================] - 0s 434us/step - loss: 0.0021\n",
            "Epoch 88/200\n",
            "4/4 [==============================] - 0s 454us/step - loss: 0.0019\n",
            "Epoch 89/200\n",
            "4/4 [==============================] - 0s 287us/step - loss: 0.0018\n",
            "Epoch 90/200\n",
            "4/4 [==============================] - 0s 285us/step - loss: 0.0017\n",
            "Epoch 91/200\n",
            "4/4 [==============================] - 0s 413us/step - loss: 0.0016\n",
            "Epoch 92/200\n",
            "4/4 [==============================] - 0s 284us/step - loss: 0.0015\n",
            "Epoch 93/200\n",
            "4/4 [==============================] - 0s 293us/step - loss: 0.0014\n",
            "Epoch 94/200\n",
            "4/4 [==============================] - 0s 290us/step - loss: 0.0014\n",
            "Epoch 95/200\n",
            "4/4 [==============================] - 0s 303us/step - loss: 0.0013\n",
            "Epoch 96/200\n",
            "4/4 [==============================] - 0s 389us/step - loss: 0.0012\n",
            "Epoch 97/200\n",
            "4/4 [==============================] - 0s 302us/step - loss: 0.0011\n",
            "Epoch 98/200\n",
            "4/4 [==============================] - 0s 429us/step - loss: 0.0011\n",
            "Epoch 99/200\n",
            "4/4 [==============================] - 0s 301us/step - loss: 9.9797e-04\n",
            "Epoch 100/200\n",
            "4/4 [==============================] - 0s 514us/step - loss: 9.3911e-04\n",
            "Epoch 101/200\n",
            "4/4 [==============================] - 0s 386us/step - loss: 8.8372e-04\n",
            "Epoch 102/200\n",
            "4/4 [==============================] - 0s 434us/step - loss: 8.3159e-04\n",
            "Epoch 103/200\n",
            "4/4 [==============================] - 0s 369us/step - loss: 7.8254e-04\n",
            "Epoch 104/200\n",
            "4/4 [==============================] - 0s 476us/step - loss: 7.3639e-04\n",
            "Epoch 105/200\n",
            "4/4 [==============================] - 0s 485us/step - loss: 6.9296e-04\n",
            "Epoch 106/200\n",
            "4/4 [==============================] - 0s 391us/step - loss: 6.5208e-04\n",
            "Epoch 107/200\n",
            "4/4 [==============================] - 0s 358us/step - loss: 6.1362e-04\n",
            "Epoch 108/200\n",
            "4/4 [==============================] - 0s 308us/step - loss: 5.7743e-04\n",
            "Epoch 109/200\n",
            "4/4 [==============================] - 0s 294us/step - loss: 5.4337e-04\n",
            "Epoch 110/200\n",
            "4/4 [==============================] - 0s 398us/step - loss: 5.1132e-04\n",
            "Epoch 111/200\n",
            "4/4 [==============================] - 0s 281us/step - loss: 4.8117e-04\n",
            "Epoch 112/200\n",
            "4/4 [==============================] - 0s 280us/step - loss: 4.5279e-04\n",
            "Epoch 113/200\n",
            "4/4 [==============================] - 0s 336us/step - loss: 4.2608e-04\n",
            "Epoch 114/200\n",
            "4/4 [==============================] - 0s 366us/step - loss: 4.0095e-04\n",
            "Epoch 115/200\n",
            "4/4 [==============================] - 0s 351us/step - loss: 3.7730e-04\n",
            "Epoch 116/200\n",
            "4/4 [==============================] - 0s 460us/step - loss: 3.5505e-04\n",
            "Epoch 117/200\n",
            "4/4 [==============================] - 0s 455us/step - loss: 3.3410e-04\n",
            "Epoch 118/200\n",
            "4/4 [==============================] - 0s 299us/step - loss: 3.1440e-04\n",
            "Epoch 119/200\n",
            "4/4 [==============================] - 0s 290us/step - loss: 2.9585e-04\n",
            "Epoch 120/200\n",
            "4/4 [==============================] - 0s 354us/step - loss: 2.7840e-04\n",
            "Epoch 121/200\n",
            "4/4 [==============================] - 0s 360us/step - loss: 2.6198e-04\n",
            "Epoch 122/200\n",
            "4/4 [==============================] - 0s 316us/step - loss: 2.4653e-04\n",
            "Epoch 123/200\n",
            "4/4 [==============================] - 0s 329us/step - loss: 2.3199e-04\n",
            "Epoch 124/200\n",
            "4/4 [==============================] - 0s 334us/step - loss: 2.1831e-04\n",
            "Epoch 125/200\n",
            "4/4 [==============================] - 0s 392us/step - loss: 2.0543e-04\n",
            "Epoch 126/200\n",
            "4/4 [==============================] - 0s 492us/step - loss: 1.9331e-04\n",
            "Epoch 127/200\n",
            "4/4 [==============================] - 0s 351us/step - loss: 1.8191e-04\n",
            "Epoch 128/200\n",
            "4/4 [==============================] - 0s 322us/step - loss: 1.7118e-04\n",
            "Epoch 129/200\n",
            "4/4 [==============================] - 0s 291us/step - loss: 1.6109e-04\n",
            "Epoch 130/200\n",
            "4/4 [==============================] - 0s 293us/step - loss: 1.5159e-04\n",
            "Epoch 131/200\n",
            "4/4 [==============================] - 0s 278us/step - loss: 1.4264e-04\n",
            "Epoch 132/200\n",
            "4/4 [==============================] - 0s 285us/step - loss: 1.3423e-04\n",
            "Epoch 133/200\n",
            "4/4 [==============================] - 0s 287us/step - loss: 1.2631e-04\n",
            "Epoch 134/200\n",
            "4/4 [==============================] - 0s 290us/step - loss: 1.1886e-04\n",
            "Epoch 135/200\n",
            "4/4 [==============================] - 0s 286us/step - loss: 1.1185e-04\n",
            "Epoch 136/200\n",
            "4/4 [==============================] - 0s 280us/step - loss: 1.0526e-04\n",
            "Epoch 137/200\n",
            "4/4 [==============================] - 0s 288us/step - loss: 9.9048e-05\n",
            "Epoch 138/200\n",
            "4/4 [==============================] - 0s 339us/step - loss: 9.3206e-05\n",
            "Epoch 139/200\n",
            "4/4 [==============================] - 0s 324us/step - loss: 8.7708e-05\n",
            "Epoch 140/200\n",
            "4/4 [==============================] - 0s 335us/step - loss: 8.2535e-05\n",
            "Epoch 141/200\n",
            "4/4 [==============================] - 0s 322us/step - loss: 7.7667e-05\n",
            "Epoch 142/200\n",
            "4/4 [==============================] - 0s 322us/step - loss: 7.3086e-05\n",
            "Epoch 143/200\n",
            "4/4 [==============================] - 0s 285us/step - loss: 6.8776e-05\n",
            "Epoch 144/200\n",
            "4/4 [==============================] - 0s 283us/step - loss: 6.4719e-05\n",
            "Epoch 145/200\n",
            "4/4 [==============================] - 0s 306us/step - loss: 6.0902e-05\n",
            "Epoch 146/200\n",
            "4/4 [==============================] - 0s 369us/step - loss: 5.7309e-05\n",
            "Epoch 147/200\n",
            "4/4 [==============================] - 0s 361us/step - loss: 5.3929e-05\n",
            "Epoch 148/200\n",
            "4/4 [==============================] - 0s 378us/step - loss: 5.0749e-05\n",
            "Epoch 149/200\n",
            "4/4 [==============================] - 0s 362us/step - loss: 4.7755e-05\n",
            "Epoch 150/200\n",
            "4/4 [==============================] - 0s 353us/step - loss: 4.4939e-05\n",
            "Epoch 151/200\n",
            "4/4 [==============================] - 0s 490us/step - loss: 4.2288e-05\n",
            "Epoch 152/200\n",
            "4/4 [==============================] - 0s 458us/step - loss: 3.9794e-05\n",
            "Epoch 153/200\n",
            "4/4 [==============================] - 0s 464us/step - loss: 3.7446e-05\n",
            "Epoch 154/200\n",
            "4/4 [==============================] - 0s 332us/step - loss: 3.5238e-05\n",
            "Epoch 155/200\n",
            "4/4 [==============================] - 0s 446us/step - loss: 3.3160e-05\n",
            "Epoch 156/200\n",
            "4/4 [==============================] - 0s 319us/step - loss: 3.1204e-05\n",
            "Epoch 157/200\n",
            "4/4 [==============================] - 0s 498us/step - loss: 2.9363e-05\n",
            "Epoch 158/200\n",
            "4/4 [==============================] - 0s 482us/step - loss: 2.7632e-05\n",
            "Epoch 159/200\n",
            "4/4 [==============================] - 0s 462us/step - loss: 2.6002e-05\n",
            "Epoch 160/200\n",
            "4/4 [==============================] - 0s 458us/step - loss: 2.4468e-05\n",
            "Epoch 161/200\n",
            "4/4 [==============================] - 0s 292us/step - loss: 2.3025e-05\n",
            "Epoch 162/200\n",
            "4/4 [==============================] - 0s 680us/step - loss: 2.1667e-05\n",
            "Epoch 163/200\n",
            "4/4 [==============================] - 0s 392us/step - loss: 2.0389e-05\n",
            "Epoch 164/200\n",
            "4/4 [==============================] - 0s 435us/step - loss: 1.9187e-05\n",
            "Epoch 165/200\n",
            "4/4 [==============================] - 0s 613us/step - loss: 1.8055e-05\n",
            "Epoch 166/200\n",
            "4/4 [==============================] - 0s 453us/step - loss: 1.6990e-05\n",
            "Epoch 167/200\n",
            "4/4 [==============================] - 0s 451us/step - loss: 1.5988e-05\n",
            "Epoch 168/200\n",
            "4/4 [==============================] - 0s 452us/step - loss: 1.5045e-05\n",
            "Epoch 169/200\n",
            "4/4 [==============================] - 0s 446us/step - loss: 1.4157e-05\n",
            "Epoch 170/200\n",
            "4/4 [==============================] - 0s 352us/step - loss: 1.3322e-05\n",
            "Epoch 171/200\n",
            "4/4 [==============================] - 0s 460us/step - loss: 1.2537e-05\n",
            "Epoch 172/200\n",
            "4/4 [==============================] - 0s 521us/step - loss: 1.1797e-05\n",
            "Epoch 173/200\n",
            "4/4 [==============================] - 0s 516us/step - loss: 1.1101e-05\n",
            "Epoch 174/200\n",
            "4/4 [==============================] - 0s 487us/step - loss: 1.0447e-05\n",
            "Epoch 175/200\n",
            "4/4 [==============================] - 0s 464us/step - loss: 9.8307e-06\n",
            "Epoch 176/200\n",
            "4/4 [==============================] - 0s 358us/step - loss: 9.2506e-06\n",
            "Epoch 177/200\n",
            "4/4 [==============================] - 0s 431us/step - loss: 8.7053e-06\n",
            "Epoch 178/200\n",
            "4/4 [==============================] - 0s 457us/step - loss: 8.1917e-06\n",
            "Epoch 179/200\n",
            "4/4 [==============================] - 0s 439us/step - loss: 7.7086e-06\n",
            "Epoch 180/200\n",
            "4/4 [==============================] - 0s 561us/step - loss: 7.2539e-06\n",
            "Epoch 181/200\n",
            "4/4 [==============================] - 0s 523us/step - loss: 6.8259e-06\n",
            "Epoch 182/200\n",
            "4/4 [==============================] - 0s 548us/step - loss: 6.4233e-06\n",
            "Epoch 183/200\n",
            "4/4 [==============================] - 0s 530us/step - loss: 6.0446e-06\n",
            "Epoch 184/200\n",
            "4/4 [==============================] - 0s 511us/step - loss: 5.6882e-06\n",
            "Epoch 185/200\n",
            "4/4 [==============================] - 0s 406us/step - loss: 5.3526e-06\n",
            "Epoch 186/200\n",
            "4/4 [==============================] - 0s 397us/step - loss: 5.0369e-06\n",
            "Epoch 187/200\n",
            "4/4 [==============================] - 0s 411us/step - loss: 4.7399e-06\n",
            "Epoch 188/200\n",
            "4/4 [==============================] - 0s 449us/step - loss: 4.4603e-06\n",
            "Epoch 189/200\n",
            "4/4 [==============================] - 0s 455us/step - loss: 4.1971e-06\n",
            "Epoch 190/200\n",
            "4/4 [==============================] - 0s 442us/step - loss: 3.9495e-06\n",
            "Epoch 191/200\n",
            "4/4 [==============================] - 0s 347us/step - loss: 3.7166e-06\n",
            "Epoch 192/200\n",
            "4/4 [==============================] - 0s 333us/step - loss: 3.4973e-06\n",
            "Epoch 193/200\n",
            "4/4 [==============================] - 0s 340us/step - loss: 3.2911e-06\n",
            "Epoch 194/200\n",
            "4/4 [==============================] - 0s 471us/step - loss: 3.0971e-06\n",
            "Epoch 195/200\n",
            "4/4 [==============================] - 0s 346us/step - loss: 2.9143e-06\n",
            "Epoch 196/200\n",
            "4/4 [==============================] - 0s 448us/step - loss: 2.7424e-06\n",
            "Epoch 197/200\n",
            "4/4 [==============================] - 0s 442us/step - loss: 2.5806e-06\n",
            "Epoch 198/200\n",
            "4/4 [==============================] - 0s 441us/step - loss: 2.4285e-06\n",
            "Epoch 199/200\n",
            "4/4 [==============================] - 0s 438us/step - loss: 2.2851e-06\n",
            "Epoch 200/200\n",
            "4/4 [==============================] - 0s 353us/step - loss: 2.1504e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndRDDAi33m26",
        "colab_type": "text"
      },
      "source": [
        "> 학습이 진행되는 동안 학습 결과와 실제 출력값의 차리를 나타내는 loss 값이 점차 줄고 있는 것을 확인할 수 있습니다. 학습이 잘 진행되고 있다는 의미겠지요. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR2GyR2FWrNu",
        "colab_type": "text"
      },
      "source": [
        "> 그럼, 모델의 학습결과, 즉 모델의 가중치를 확인해 볼까요? \n",
        "w값은 거의 -1, b 값은 거의 1이 나오는 것을 알 수 있지요? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh2ieLwMWQI2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "62236574-1004-49a0-8316-c4ccbab3a601"
      },
      "source": [
        "model.weights"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'dense_1/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[-0.99881613]], dtype=float32)>,\n",
              " <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([0.99651915], dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXD5XdLi36wg",
        "colab_type": "text"
      },
      "source": [
        "> 학습이 진행되는 동안(epoch) 손실값(실제값과 학습값과의 차이)의 변화를 확인해 보겠습니다. 그래프를 그리는 도구인 matplotlib 모듈을 사용하겠습니다. \n",
        "\n",
        "> 학습이 진행되는 동안 손실값은 history.history라는 변수에 dictionary 형태로 저장되어 있습니다. 따라서 여러분은 다음과 같이 100번의 반복동안 손실함수의 값(mse)을 확인할 수 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Jy9JN_M4eCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaQdQGb3_yiB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "3746a53d-ec40-4e23-c8fd-7d70b55d3165"
      },
      "source": [
        "plt.plot(history.history[\"loss\"])\n",
        "plt.title(\"Model loss\")\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRBsXuB_V8pO",
        "colab_type": "text"
      },
      "source": [
        "> 학습이 끝났으니, 이번에는 새로운 데이터(샘플)에 대한 모델의 예측값을 확인해 볼까요? \n",
        "만일 입력데이터 5라는 새로운 샘플이 주어졌을때, 출력은 어떻게 될까요? \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rik3PJGV8x-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}